{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Q1. What is Min-Max scaling, and how is it used in data preprocessing? Provide an example to illustrate its\n",
    "application.\n",
    "Q2. What is the Unit Vector technique in feature scaling, and how does it differ from Min-Max scaling?\n",
    "Provide an example to illustrate its application.\n",
    "Q3. What is PCA (Principle Component Analysis), and how is it used in dimensionality reduction? Provide an\n",
    "example to illustrate its application.\n",
    "Q4. What is the relationship between PCA and Feature Extraction, and how can PCA be used for Feature\n",
    "Extraction? Provide an example to illustrate this concept.\n",
    "Q5. You are working on a project to build a recommendation system for a food delivery service. The dataset\n",
    "contains features such as price, rating, and delivery time. Explain how you would use Min-Max scaling to\n",
    "preprocess the data.\n",
    "Q6. You are working on a project to build a model to predict stock prices. The dataset contains many\n",
    "features, such as company financial data and market trends. Explain how you would use PCA to reduce the\n",
    "dimensionality of the dataset.\n",
    "Q7. For a dataset containing the following values: [1, 5, 10, 15, 20], perform Min-Max scaling to transform the\n",
    "values to a range of -1 to 1.\n",
    "Q8. For a dataset containing the following features: [height, weight, age, gender, blood pressure], perform\n",
    "Feature Extraction using PCA. How many principal components would you choose to retain, and why?"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Q1. Min-Max scaling, also known as normalization, is a data preprocessing technique used to scale numeric features to a specific range. It transforms the original values of the feature into a normalized range between a minimum and maximum value, typically between 0 and 1.\n",
    "\n",
    "The formula for Min-Max scaling is:\n",
    "scaled_value = (original_value - min_value) / (max_value - min_value)\n",
    "\n",
    "For example, let's say we have a dataset of house prices with the following values: [100,000, 200,000, 300,000, 400,000]. To apply Min-Max scaling, we determine the minimum and maximum values of the dataset, which are 100,000 and 400,000, respectively. Then, we scale each value using the formula mentioned earlier:\n",
    "\n",
    "scaled_value = (original_value - min_value) / (max_value - min_value)\n",
    "\n",
    "For the first value (100,000):\n",
    "scaled_value = (100,000 - 100,000) / (400,000 - 100,000) = 0\n",
    "\n",
    "For the second value (200,000):\n",
    "scaled_value = (200,000 - 100,000) / (400,000 - 100,000) = 0.3333\n",
    "\n",
    "For the third value (300,000):\n",
    "scaled_value = (300,000 - 100,000) / (400,000 - 100,000) = 0.6667\n",
    "\n",
    "For the fourth value (400,000):\n",
    "scaled_value = (400,000 - 100,000) / (400,000 - 100,000) = 1\n",
    "\n",
    "So, the scaled values range from 0 to 1, representing the original house prices in a normalized range.\n",
    "\n",
    "\n",
    "\n",
    "Q2. The Unit Vector technique, also known as normalization or feature scaling, is another data preprocessing technique used to scale numeric features. It transforms the original values of a feature into a unit vector, meaning the vector's magnitude becomes 1 while preserving its direction.\n",
    "\n",
    "The formula for the Unit Vector technique is:\n",
    "unit_vector = original_value / magnitude\n",
    "\n",
    "To calculate the magnitude of a vector, we use the Euclidean norm:\n",
    "magnitude = sqrt(sum(original_value^2))\n",
    "\n",
    "For example, let's say we have a dataset of movie ratings with the following values: [2, 4, 6, 8]. To apply the Unit Vector technique, we calculate the magnitude of the vector:\n",
    "\n",
    "magnitude = sqrt(2^2 + 4^2 + 6^2 + 8^2) = sqrt(4 + 16 + 36 + 64) = sqrt(120) = 10.954\n",
    "\n",
    "Then, we scale each value using the formula mentioned earlier:\n",
    "\n",
    "unit_vector = original_value / magnitude\n",
    "\n",
    "For the first value (2):\n",
    "unit_vector = 2 / 10.954 = 0.182\n",
    "\n",
    "For the second value (4):\n",
    "unit_vector = 4 / 10.954 = 0.364\n",
    "\n",
    "For the third value (6):\n",
    "unit_vector = 6 / 10.954 = 0.546\n",
    "\n",
    "For the fourth value (8):\n",
    "unit_vector = 8 / 10.954 = 0.728\n",
    "\n",
    "So, the scaled values are unit vectors with magnitudes of 1, representing the original movie ratings.\n",
    "\n",
    "\n",
    "Q3. PCA (Principal Component Analysis) is a dimensionality reduction technique used to transform a high-dimensional dataset into a lower-dimensional space. It identifies the directions (principal components) along which the data varies the most and projects the data onto these components while preserving the maximum amount of information.\n",
    "\n",
    "In PCA, the principal components are computed based on the covariance matrix of the dataset. The first principal component captures the largest amount of variance in the data, followed by the second principal component, and so on. By selecting a subset of these components, we can reduce the dimensionality of the dataset while retaining a significant amount of its information.\n",
    "\n",
    "For example, suppose we have a dataset with multiple features representing the properties of houses, such as area, number of rooms, and number of bathrooms. By applying PCA, we can determine the most significant components that explain the majority of the variance in the data. We can then project the original dataset onto these components, effectively reducing the dimensionality while retaining the essential characteristics of the houses.\n",
    "\n",
    "\n",
    "Q4. PCA can be used for feature extraction in the sense that it extracts a set of new features, called principal components, from the original features. These principal components are linear combinations of the original features and are selected in a way that maximizes the amount of variance explained by them.\n",
    "\n",
    "By using PCA for feature extraction, we can reduce the dimensionality of the dataset while still retaining most of the important information. The new set of features (principal components) can be used as input for subsequent machine learning models.\n",
    "\n",
    "For example, let's consider a dataset with several numerical features representing the physical properties of cars, such as horsepower, weight, and engine displacement. By applying PCA to this dataset, we can extract a smaller set of principal components that capture the most significant variations in the data. These components might represent higher-level characteristics of the cars, such as power-to-weight ratio or overall performance. By using these principal components as features, we effectively perform feature extraction, reducing the dimensionality of the dataset while preserving the essential information.\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "Q5. In the context of building a recommendation system for a food delivery service, Min-Max scaling can be used to preprocess the data, specifically the features such as price, rating, and delivery time.\n",
    "\n",
    "Min-Max scaling would be applied to these features to transform their values into a normalized range, typically between 0 and 1. This normalization is beneficial because it ensures that each feature contributes proportionately to the recommendation system, regardless of their original scales.\n",
    "\n",
    "For example, suppose we have a dataset with food items and their corresponding features. The price ranges from $5 to $30, the rating ranges from 1 to 5, and the delivery time ranges from 20 minutes to 60 minutes. By applying Min-Max scaling, we can transform these features into a normalized range of 0 to 1, making them comparable and avoiding any bias caused by their original scales. This preprocessing step ensures that price, rating, and delivery time are equally considered in the recommendation system, enabling fair and accurate recommendations.\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "Q6. When building a model to predict stock prices with a dataset containing multiple features, including company financial data and market trends, PCA can be used to reduce the dimensionality of the dataset.\n",
    "\n",
    "To use PCA for dimensionality reduction in this scenario, the first step would involve applying Min-Max scaling or standardization to normalize the features, making them comparable and removing any potential bias due to their original scales.\n",
    "\n",
    "Next, PCA would be applied to the preprocessed dataset. The goal is to identify the principal components that capture the most significant variations in the data. These principal components would represent combinations of the original features that explain the majority of the variance in the dataset. By selecting a subset of these components, we can effectively reduce the dimensionality of the dataset while retaining the essential information.\n",
    "\n",
    "For instance, the original dataset might contain features such as stock price history, company revenue, market index values, and news sentiment scores. By applying PCA, we can extract a smaller set of principal components that summarize the main trends and patterns in the data. These components could represent market performance, company financial strength, or other relevant factors. By reducing the dimensionality using PCA, we simplify the dataset and potentially improve the model's performance by focusing on the most informative features.\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.0, 0.21052631578947367, 0.47368421052631576, 0.7368421052631579, 1.0]\n"
     ]
    }
   ],
   "source": [
    "'''Q7. For a dataset containing the following values: [1, 5, 10, 15, 20], perform Min-Max scaling to transform the\n",
    "values to a range of -1 to 1.'''\n",
    "da=[1,5,10,15,20]\n",
    "min_value=1\n",
    "max_value=20\n",
    "minmax=[]\n",
    "for i in da:\n",
    "    minmax.append((i-min_value)/(max_value-min_value))\n",
    "print(minmax)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "'''Q8. For a dataset containing the following features: [height, weight, age, gender, blood pressure], perform\n",
    "Feature Extraction using PCA. How many principal components would you choose to retain, and why?\n",
    "\n",
    "When performing Feature Extraction using PCA on a dataset with features [height, weight, age, gender, blood pressure], \n",
    "the number of principal components to retain depends on the desired trade-off between dimensionality reduction and preserving information.\n",
    "\n",
    "To determine the appropriate number of principal components to retain, one approach is to analyze the explained variance ratio. \n",
    "This ratio indicates the amount of variance in the data explained by each principal component. By summing up the explained variance ratios of the principal components, we can determine the cumulative amount of variance explained.\n",
    "\n",
    "A common rule of thumb is to select the number of principal components that explain a significant portion of the variance, such as 95% or 99%. This ensures that most of the important information is retained while reducing the dimensionality.\n",
    "\n",
    "In practice, we can use scree plots or cumulative variance plots to visualize the explained variance ratio and select the appropriate number of principal components based on the desired variance threshold.\n",
    "\n",
    "For example, let's say that after performing PCA on the dataset, we find that the first three principal components explain 90% of the variance. In this case, we might choose to retain these three components, as they capture a substantial amount of the information in the original dataset while reducing its dimensionality. However, the final decision would depend on the specific requirements of the problem and the desired trade-off between dimensionality reduction and information preservation.\n",
    "\n",
    "'''\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.1"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
